{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decentralized Path Planning - Mini Problem Set\n",
    "\n",
    "### Due May xth, 11:59 p.m.\n",
    "\n",
    "In this problem set, you'll implement **DMA-RRT** and **DMA-RRT Collaborative** and use it to solve decentralized multiagent path planning problems. Specifically, you will apply it to the VIPER path planning scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of Contents\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you load the following dependencies by highlighting the cell below and pressing `Shift + Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from dma_rrt import *\n",
    "from utils import *\n",
    "from other import *  # TODO(marcus): fix these imports to be correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "In brief, NASA's current architecture for lunar exploration is a gradual ramp up of capabilities. The first foray, the Volatiles Investigating Polar Exploration Rover, will launch in the 2022 timeframe with the express mission of mapping water-ice at potential Artemis landing sites. From this perspective, we can extrapolate to a concept of operations for planetary exploration wherein teams of rovers are responsible for advanced scouting. If humans are dependent on their results, time will be of the essence.\n",
    "\n",
    "Furthermore, novel environments will introduce communication challenges. There may not be resources to run central authorities. Ground communications may be limited, spotty, or massively delayed.\n",
    "\n",
    "Given all the constraints described above, we need to consider algorithms that allow teams of agents to distribute decision making online. For this problem set, you will address the problem of distributed path planning.\n",
    "\n",
    "See the tutorial for more information.\n",
    "\n",
    "## Modeling\n",
    "\n",
    "We've provided some tools to make implementation easier. In particular, we have provided you with a working implementation of Closed-Loop Rapidly Exploring Random Trees (CL-RRT), which is the underlying base planner for each agent in the scenario. You will be responsible for implementing the interactive and cooperative components between agents.\n",
    "\n",
    "In addition, there are helper functions and vizualization tools provided to help with implementing the core (Decentralized, Multi-Agent RRT) DMA-RRT algorithms. If you're curious how the code, works, feel free to check out the source files provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Example\n",
    "\n",
    "Consider the following problem:\n",
    "\n",
    "> A VIPER-type rover is in a potential science site. We will assume car-like dynamics for the rover. It needs to move from location $\\alpha$ at (0, 0) to the goal site, $\\beta$ at (10, 6).\n",
    "\n",
    "Let's model this problem with the API we provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(ReedsSheppDynamics())\n",
    "env = Environment(\"../utils/simple.yaml\")\n",
    "\n",
    "agent.set_start([0, 0])\n",
    "agent.set_goal([10, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully modeled the problem; we've instantiated an `agent` with car-like dynamics, as well as an environment (2D map) in which the agent is operating. We've given the agent a start and end point for planning as well. Now all that's left is to encapsulate the problem in a `Plan` object, which works for both the single and multi-agent scenarios. We will let the program spin up and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = PlanMono([car])\n",
    "plan.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.spin()\n",
    "plan.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling API\n",
    "\n",
    "Here is a table showing all the modeling options at your disposal:\n",
    "\n",
    "TODO: add table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling VIPER\n",
    "\n",
    "Here you will model the VIPER multi-agent scenario using the tools provided. For help with the API, refer to the above API table. \n",
    "\n",
    "The scenario is as follows:\n",
    "\n",
    ">You are planning a VIPER-esque mission to explore the Lunar surface. You have three rovers on the ground, all of which are identical in their dynamics, size, and shape. These rovers have all landed on the surface in different locations, and they have been pre-assigned goal locations for sampling the soil. The agents must autonomously plan their paths to their respective goal locations without colliding with known boulders or falling into craters (both represented as static obstacles in the 2D map). They must also not collide with each other.\n",
    "\n",
    "To model this scenario, you must create the agents, assign them their dynamics models, and give them start and end positions. The environment is pre-loaded for you with the `moon_env` variable. All of this must be encapsulated in a `Plan` object named `plan`.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Complete the model below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_env = Environment(\"../utils/moon_env.yaml\")\n",
    "\n",
    "# agent1 = Agent(...)\n",
    "# ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "\n",
    "plan = Plan()  # Modify this!\n",
    "plan.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your model here. This code will run a few tests against your model, but isn't exhaustive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_viper(plan)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement DMA-RRT\n",
    "\n",
    "We have provided you with CL-RRT for each agent, but as we dicussed in lecture, having each agent run CL-RRT on its own asynchronously can lead to collisions between agents. We must therefore implement DMA-RRT such that the agents can plan as a team and avoid collisions.\n",
    "\n",
    "The `Agent` class also has an `Antenna` attribute that allows it to communicate with other agents. This is fully implemented for you; you can use the following APIs to transmit and receive messages to and from other agents:\n",
    "\n",
    "TODO: add Antenna class API table here!\n",
    "\n",
    "The `Agent` class also has an internal CL-RRT planner that you will need to interface with. Here is the API list for that class:\n",
    "\n",
    "TODO: talk more about how you use this API!\n",
    "TODO: add RRT class API table here!\n",
    "\n",
    "DMA-RRT has two components. The individual component handles internal RRT tree grown, choosing the best path, and sending messages to other agents. The interaction component handles inter-agent communications and updating internal constraints. Refer to the tutorial for more information on these algorithms.\n",
    "\n",
    "Implement DMA-RRRT individual and interaction as two separate functions below. Refer to the docstring for more information on input args and return types.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Complete the functions below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dma_rrt_individual(self, agent):\n",
    "    \"\"\" TODO: add docstring!\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    \n",
    "def dma_rrt_interaction(self, agent):\n",
    "    \"\"\" TODO: add docstring!\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to visualize your planner in real time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.dma_rrt_indiv = dma_rrt_individual\n",
    "plan.dma_rrt_inter = dma_rrt_interaction\n",
    "\n",
    "plan.spin(viz=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_plan_viper(plan)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement DMA-RRT Collaborative\n",
    "\n",
    "DMA-RRT is great, but doesn't provide global optimality guarantees. If we want a scheme that does better, we must implement the \"Collaborative\" extension as described in the tutorial and the original paper (TODO: link to paper here!)\n",
    "\n",
    "Implement the Collaborative extension below.\n",
    "\n",
    "TODO: add additional API info as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dma_rrt_collaborative(self, agent):\n",
    "    \"\"\" TODO: add docstring!\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to visualize your planner in real time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.dma_rrt_collab = dma_rrt_collaborative\n",
    "\n",
    "plan.spin(viz=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_plan_viper_collab(plan)\n",
    "test_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "We can now see that the planners both work and produce feasible, correct paths for each agent. How can we quantify the improvement that the Collaborative Extension has granted us? Run the following cells to evaluate multiple runs of both algorithms and visualize their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add analysis code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the trends you see. Is Collaborative better? If so, by how much? Why do you think it performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the limitations you can see in this set of algorithms. How does it scale with the different mission parameters? What are the underlying assumptions of the model that may not be reflected in reality? Can you think of a way to address those assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **You're Done!**\n",
    "Don't forget to validate your notebook and submit."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
